{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recursive Language Models (RLM): A Hands-On Tutorial\n",
    "\n",
    "Welcome! This notebook introduces **Recursive Language Models (RLM)** — an inference paradigm that lets language models programmatically examine, decompose, and recursively call themselves over their input using a REPL environment.\n",
    "\n",
    "Instead of a single `llm.completion()` call, RLM gives the model a **code execution environment** where it can write and run code, inspect intermediate results, and spawn **recursive sub-calls** to itself to handle sub-problems — enabling near-infinite context handling.\n",
    "\n",
    "> **Paper:** *Recursive Language Models* — Alex L. Zhang, Tim Kraska, Omar Khattab (MIT, 2025)  \n",
    "> **Repository:** [github.com/alexzhang13/rlm](https://github.com/alexzhang13/rlm)\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Setup & Configuration](#1-setup--configuration)\n",
    "2. [What is an RLM?](#2-what-is-an-rlm)\n",
    "3. [Your First RLM Completion](#3-your-first-rlm-completion)\n",
    "4. [How the REPL Loop Works](#4-how-the-repl-loop-works)\n",
    "5. [Recursive Sub-Calls: Depth > 1](#5-recursive-sub-calls)\n",
    "6. [Batched Recursive Queries](#6-batched-recursive-queries)\n",
    "7. [Context Compaction](#7-context-compaction)\n",
    "8. [Custom Tools](#8-custom-tools)\n",
    "9. [Logging & Trajectory Inspection](#9-logging--trajectory-inspection)\n",
    "10. [Exercises](#10-exercises)\n",
    "11. [References](#11-references)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup & Configuration <a id='1-setup--configuration'></a>\n",
    "\n",
    "RLM requires an LLM backend. This tutorial is pre-configured to use the API key provided by your instructor via environment variables. If you're running this on your own, set the appropriate key below.\n",
    "\n",
    "### Supported backends\n",
    "\n",
    "| Backend | Environment Variable | Example Model |\n",
    "|---------|---------------------|---------------|\n",
    "| OpenAI | `OPENAI_API_KEY` | `gpt-4o`, `gpt-4o-mini` |\n",
    "| Anthropic | `ANTHROPIC_API_KEY` | `claude-sonnet-4-20250514` |\n",
    "| Portkey (router) | `PORTKEY_API_KEY` | `@openai/gpt-4o` |\n",
    "| OpenRouter | `OPENROUTER_API_KEY` | `openai/gpt-4o` |\n",
    "| vLLM (local) | `VLLM_BASE_URL` | your local model |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# ── Pick your backend ──────────────────────────────────────────────\n",
    "# The environment variables are pre-set by the JupyterHub deployment.\n",
    "# If they aren't set, uncomment ONE of the lines below and paste your key.\n",
    "\n",
    "# os.environ[\"OPENAI_API_KEY\"]    = \"sk-...\"\n",
    "# os.environ[\"ANTHROPIC_API_KEY\"] = \"sk-ant-...\"\n",
    "# os.environ[\"PORTKEY_API_KEY\"]   = \"...\"\n",
    "\n",
    "# ── Detect which backend is available ──────────────────────────────\n",
    "if os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    BACKEND = \"openai\"\n",
    "    BACKEND_KWARGS = {\n",
    "        \"model_name\": os.environ.get(\"OPENAI_MODEL\", \"gpt-4o-mini\"),\n",
    "        \"api_key\": os.environ[\"OPENAI_API_KEY\"],\n",
    "    }\n",
    "elif os.environ.get(\"ANTHROPIC_API_KEY\"):\n",
    "    BACKEND = \"anthropic\"\n",
    "    BACKEND_KWARGS = {\n",
    "        \"model_name\": os.environ.get(\"ANTHROPIC_MODEL\", \"claude-sonnet-4-20250514\"),\n",
    "        \"api_key\": os.environ[\"ANTHROPIC_API_KEY\"],\n",
    "    }\n",
    "elif os.environ.get(\"PORTKEY_API_KEY\"):\n",
    "    BACKEND = \"portkey\"\n",
    "    BACKEND_KWARGS = {\n",
    "        \"model_name\": os.environ.get(\"PORTKEY_MODEL\", \"@openai/gpt-4o-mini\"),\n",
    "        \"api_key\": os.environ[\"PORTKEY_API_KEY\"],\n",
    "    }\n",
    "else:\n",
    "    raise EnvironmentError(\n",
    "        \"No API key found. Set one of: OPENAI_API_KEY, ANTHROPIC_API_KEY, \"\n",
    "        \"or PORTKEY_API_KEY in this cell or in the JupyterHub environment.\"\n",
    "    )\n",
    "\n",
    "print(f\"Backend:  {BACKEND}\")\n",
    "print(f\"Model:    {BACKEND_KWARGS['model_name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the rlm library is installed\n",
    "from rlm import RLM\n",
    "from rlm.logger import RLMLogger\n",
    "\n",
    "print(\"rlm library imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. What is an RLM? <a id='2-what-is-an-rlm'></a>\n",
    "\n",
    "### The Problem with Standard LLM Calls\n",
    "\n",
    "A normal LLM call looks like:\n",
    "\n",
    "```python\n",
    "response = llm.completion(\"Summarize this 500-page document: ...\")\n",
    "```\n",
    "\n",
    "This has fundamental limitations:\n",
    "- **Context window limits** — the document may not fit in the model's context\n",
    "- **Single-shot reasoning** — the model must produce its answer in one pass\n",
    "- **No tool use** — the model can't inspect, slice, or compute over the input\n",
    "\n",
    "### The RLM Solution\n",
    "\n",
    "An RLM call replaces this with an **iterative REPL loop**:\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────┐\n",
    "│                   RLM Loop                       │\n",
    "│                                                  │\n",
    "│  1. Model receives prompt + context variable     │\n",
    "│  2. Model writes code in a ```python block       │\n",
    "│  3. Code executes in REPL environment            │\n",
    "│  4. stdout/stderr fed back to model              │\n",
    "│  5. Repeat until model calls FINAL_VAR(answer)   │\n",
    "│                                                  │\n",
    "│  At any step, the model can:                     │\n",
    "│  • Slice/index the context variable              │\n",
    "│  • Spawn sub-RLM calls with rlm_query()          │\n",
    "│  • Store intermediate results in REPL variables  │\n",
    "│  • Use any Python library available              │\n",
    "└─────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### Key Built-in Functions\n",
    "\n",
    "Inside the REPL, the model has access to:\n",
    "\n",
    "| Function | Purpose |\n",
    "|----------|--------|\n",
    "| `context` | Variable holding the input payload |\n",
    "| `FINAL_VAR(x)` | Declare the final answer and exit the loop |\n",
    "| `llm_query(prompt)` | Query the underlying LLM directly (no REPL) |\n",
    "| `rlm_query(prompt)` | Spawn a recursive child RLM (with its own REPL) |\n",
    "| `rlm_query_batched(prompts)` | Spawn multiple child RLMs in parallel |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Your First RLM Completion <a id='3-your-first-rlm-completion'></a>\n",
    "\n",
    "Let's start with the simplest possible RLM call. The model will use the REPL to compute something and return it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an RLM instance\n",
    "rlm = RLM(\n",
    "    backend=BACKEND,\n",
    "    backend_kwargs=BACKEND_KWARGS,\n",
    "    environment=\"local\",       # Execute code in the local Python process\n",
    "    environment_kwargs={},\n",
    "    max_depth=1,               # No recursive sub-calls (yet)\n",
    "    verbose=True,              # Print the REPL interaction to console\n",
    ")\n",
    "\n",
    "print(\"RLM instance created.\")\n",
    "print(f\"  Backend: {BACKEND}\")\n",
    "print(f\"  Environment: local\")\n",
    "print(f\"  Max depth: 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── First completion: a simple computation ──\n",
    "result = rlm.completion(\n",
    "    \"Compute the sum of the first 10 Fibonacci numbers. \"\n",
    "    \"Show your work step by step in the REPL, then return the answer with FINAL_VAR.\"\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RESULT\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Response: {result.response}\")\n",
    "print(f\"Execution time: {result.execution_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What just happened?\n",
    "\n",
    "1. The RLM sent your prompt to the LLM backend\n",
    "2. The model responded with a Python code block (in \\`\\`\\`python fences)\n",
    "3. RLM extracted the code and executed it in a sandboxed REPL\n",
    "4. The stdout was fed back to the model as the next message\n",
    "5. The model eventually called `FINAL_VAR(answer)` to return its result\n",
    "\n",
    "The `verbose=True` flag let you see the full back-and-forth. This iterative loop is the core of RLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. How the REPL Loop Works <a id='4-how-the-repl-loop-works'></a>\n",
    "\n",
    "Let's look more carefully at the iteration mechanics. We'll use the **RLMLogger** to capture the full trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an RLM with logging\n",
    "logger = RLMLogger()  # In-memory logger\n",
    "\n",
    "rlm_logged = RLM(\n",
    "    backend=BACKEND,\n",
    "    backend_kwargs=BACKEND_KWARGS,\n",
    "    environment=\"local\",\n",
    "    max_depth=1,\n",
    "    max_iterations=5,  # Safety limit on REPL turns\n",
    "    logger=logger,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "result = rlm_logged.completion(\n",
    "    \"Generate a list of 20 random integers between 1 and 100 using random.seed(42). \"\n",
    "    \"Compute the mean, median, and standard deviation. \"\n",
    "    \"Print each result clearly, then return a dict with all three values via FINAL_VAR.\"\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"Response: {result.response}\")\n",
    "print(f\"Execution time: {result.execution_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Inspect the trajectory ──\n",
    "if result.metadata:\n",
    "    traj = result.metadata\n",
    "    iterations = traj.get(\"iterations\", [])\n",
    "    print(f\"Total iterations: {len(iterations)}\")\n",
    "    print()\n",
    "    \n",
    "    for i, iteration in enumerate(iterations):\n",
    "        print(f\"── Iteration {i + 1} ──\")\n",
    "        code_blocks = iteration.get(\"code_blocks\", [])\n",
    "        for j, cb in enumerate(code_blocks):\n",
    "            code = cb.get(\"code\", \"\")\n",
    "            repl_result = cb.get(\"result\", {})\n",
    "            stdout = repl_result.get(\"stdout\", \"\")\n",
    "            stderr = repl_result.get(\"stderr\", \"\")\n",
    "            \n",
    "            print(f\"  Code block {j + 1}:\")\n",
    "            # Show first 300 chars of code\n",
    "            for line in code.split(\"\\n\")[:10]:\n",
    "                print(f\"    >>> {line}\")\n",
    "            if len(code.split(\"\\n\")) > 10:\n",
    "                print(f\"    ... ({len(code.split(chr(10)))} lines total)\")\n",
    "            if stdout:\n",
    "                print(f\"  stdout: {stdout[:200]}\")\n",
    "            if stderr:\n",
    "                print(f\"  stderr: {stderr[:200]}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"No trajectory metadata captured.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Observations\n",
    "\n",
    "Notice how the model:\n",
    "1. **Wrote Python code** to generate the data and compute statistics\n",
    "2. **Used `print()`** to inspect intermediate results\n",
    "3. **Called `FINAL_VAR()`** when it had the final answer\n",
    "4. **Persisted state** across REPL iterations (variables survive between turns)\n",
    "\n",
    "This is fundamentally different from chain-of-thought or tool-use — the model has **full programmatic control** over its reasoning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Recursive Sub-Calls: Depth > 1 <a id='5-recursive-sub-calls'></a>\n",
    "\n",
    "The **recursive** part of RLM comes from `rlm_query()`. When the model calls `rlm_query(prompt)` inside its REPL, a **child RLM** is spawned with its own REPL environment. The child processes the sub-problem independently and returns its result to the parent.\n",
    "\n",
    "```\n",
    "Parent RLM (depth=0)\n",
    "  │\n",
    "  ├── Iteration 1: writes code, calls rlm_query(\"sub-task\")\n",
    "  │     │\n",
    "  │     └── Child RLM (depth=1)\n",
    "  │           ├── Iteration 1: writes code, prints results\n",
    "  │           └── Iteration 2: calls FINAL_VAR(answer)\n",
    "  │                             └── answer flows back to parent\n",
    "  │\n",
    "  └── Iteration 2: uses child's answer, calls FINAL_VAR(final)\n",
    "```\n",
    "\n",
    "This enables **divide-and-conquer** over arbitrarily long inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Recursive RLM: depth=2 ──\n",
    "logger_deep = RLMLogger()\n",
    "\n",
    "rlm_deep = RLM(\n",
    "    backend=BACKEND,\n",
    "    backend_kwargs=BACKEND_KWARGS,\n",
    "    environment=\"local\",\n",
    "    max_depth=2,           # Allow one level of recursion\n",
    "    max_iterations=5,\n",
    "    logger=logger_deep,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "prompt = (\n",
    "    \"Use rlm_query() to ask a sub-model: \"\n",
    "    \"'What are the first 8 prime numbers? Reply with just the numbers separated by commas.' \"\n",
    "    \"Store the response in a variable called 'primes'. \"\n",
    "    \"Then parse the primes into a list of integers, compute their sum, \"\n",
    "    \"and return the sum with FINAL_VAR.\"\n",
    ")\n",
    "\n",
    "result = rlm_deep.completion(prompt)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"Response: {result.response}\")\n",
    "print(f\"Execution time: {result.execution_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Inspect sub-call metadata ──\n",
    "def print_metadata_tree(result, depth=0):\n",
    "    \"\"\"Recursively print metadata from an RLMChatCompletion.\"\"\"\n",
    "    indent = \"  \" * depth\n",
    "    prefix = \"└─ \" if depth > 0 else \"\"\n",
    "    \n",
    "    print(f\"{indent}{prefix}[Depth {depth}] \"\n",
    "          f\"time={result.execution_time:.2f}s  \"\n",
    "          f\"response_len={len(str(result.response))}\")\n",
    "    \n",
    "    if result.metadata:\n",
    "        traj = result.metadata\n",
    "        n_iters = len(traj.get(\"iterations\", []))\n",
    "        print(f\"{indent}   {n_iters} iteration(s)\")\n",
    "        \n",
    "        for i, iteration in enumerate(traj.get(\"iterations\", [])):\n",
    "            for cb in iteration.get(\"code_blocks\", []):\n",
    "                repl_result = cb.get(\"result\", {})\n",
    "                for j, sub_call in enumerate(repl_result.get(\"rlm_calls\", [])):\n",
    "                    sub_resp = str(sub_call.get(\"response\", \"\"))[:80]\n",
    "                    print(f\"{indent}   iter {i+1} sub-call {j+1}: \"\n",
    "                          f\"response={sub_resp!r}\")\n",
    "    print()\n",
    "\n",
    "print(\"METADATA TREE\")\n",
    "print(\"=\" * 40)\n",
    "print_metadata_tree(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Batched Recursive Queries <a id='6-batched-recursive-queries'></a>\n",
    "\n",
    "When the model needs to process multiple sub-tasks, it can use `rlm_query_batched()` to spawn several child RLMs **in parallel**. This is crucial for tasks like summarizing multiple document sections simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Batched parallel sub-calls ──\n",
    "rlm_batched = RLM(\n",
    "    backend=BACKEND,\n",
    "    backend_kwargs=BACKEND_KWARGS,\n",
    "    environment=\"local\",\n",
    "    max_depth=2,\n",
    "    max_iterations=5,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "prompt = (\n",
    "    \"Use rlm_query_batched() to ask THREE different questions in parallel:\\n\"\n",
    "    \"  1. 'What are the first 5 prime numbers? Reply with just the numbers.'\\n\"\n",
    "    \"  2. 'What are the first 5 perfect squares? Reply with just the numbers.'\\n\"\n",
    "    \"  3. 'What are the first 5 triangular numbers? Reply with just the numbers.'\\n\"\n",
    "    \"Store the list of responses in a variable called 'answers', \"\n",
    "    \"then combine them into a summary string and return it with FINAL_VAR.\"\n",
    ")\n",
    "\n",
    "result = rlm_batched.completion(prompt)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"Response: {result.response}\")\n",
    "print(f\"Execution time: {result.execution_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why batched queries matter\n",
    "\n",
    "Consider summarizing a 500-page document:\n",
    "\n",
    "1. The parent RLM **chunks** the document (stored in `context`) into sections\n",
    "2. It calls `rlm_query_batched()` with a summarization prompt for each chunk\n",
    "3. Each child RLM summarizes its chunk independently **in parallel**\n",
    "4. The parent collects all summaries and produces the final result\n",
    "\n",
    "This recursive divide-and-conquer pattern is what gives RLM its power over arbitrarily long inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Context Compaction <a id='7-context-compaction'></a>\n",
    "\n",
    "When an RLM runs for many iterations, the conversation history can fill up the context window. **Compaction** automatically summarizes the conversation when usage exceeds a threshold, preserving key information while freeing space.\n",
    "\n",
    "The model receives a `history` variable containing the summarized prior context, so it can still reference earlier results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Compaction example ──\n",
    "# We use a very low threshold to force compaction to trigger\n",
    "\n",
    "rlm_compact = RLM(\n",
    "    backend=BACKEND,\n",
    "    backend_kwargs=BACKEND_KWARGS,\n",
    "    environment=\"local\",\n",
    "    max_depth=1,\n",
    "    max_iterations=12,\n",
    "    compaction=True,\n",
    "    compaction_threshold_pct=0.02,  # Very low — triggers quickly for demo\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "prompt = (\n",
    "    \"Complete the following steps, each in its own REPL block (one block per message). \"\n",
    "    \"Do NOT combine steps.\\n\\n\"\n",
    "    \"Step 1: Use `import random; random.seed(42)` then generate a list of 20 random \"\n",
    "    \"integers between 1 and 1000. Print ALL of them. Store as `data_a`.\\n\\n\"\n",
    "    \"Step 2: Generate another 20 random integers (continuing the same RNG stream) \"\n",
    "    \"between 1 and 1000. Print ALL of them. Store as `data_b`.\\n\\n\"\n",
    "    \"Step 3: Compute and print the mean of `data_a`. Store as `mean_a`.\\n\\n\"\n",
    "    \"Step 4: Compute and print the mean of `data_b`. Store as `mean_b`.\\n\\n\"\n",
    "    \"Step 5: Compute final_answer = round(mean_a + mean_b, 2) and call FINAL_VAR(final_answer).\"\n",
    ")\n",
    "\n",
    "result = rlm_compact.completion(prompt, root_prompt=prompt)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"Response: {result.response}\")\n",
    "print(f\"Execution time: {result.execution_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How compaction works\n",
    "\n",
    "1. After each iteration, RLM checks what percentage of the context window is used\n",
    "2. When usage exceeds `compaction_threshold_pct`, the conversation history is **summarized** by the LLM\n",
    "3. The summary replaces the full history, freeing context space\n",
    "4. The model receives a `history` variable with the summary so it can still reference past results\n",
    "5. REPL variables (like `data_a`, `mean_a`) persist in the environment regardless of compaction\n",
    "\n",
    "This is crucial for tasks that require many iterations — without compaction, the context window would fill up and the model would fail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Custom Tools <a id='8-custom-tools'></a>\n",
    "\n",
    "You can inject **custom functions and data** into the RLM's REPL environment. The model can then call these functions as part of its reasoning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "# Define some custom tools\n",
    "def word_count(text: str) -> int:\n",
    "    \"\"\"Count the number of words in a text.\"\"\"\n",
    "    return len(text.split())\n",
    "\n",
    "def extract_numbers(text: str) -> list[float]:\n",
    "    \"\"\"Extract all numbers from a text string.\"\"\"\n",
    "    import re\n",
    "    return [float(x) for x in re.findall(r'-?\\d+\\.?\\d*', text)]\n",
    "\n",
    "def celsius_to_fahrenheit(c: float) -> float:\n",
    "    \"\"\"Convert Celsius to Fahrenheit.\"\"\"\n",
    "    return c * 9/5 + 32\n",
    "\n",
    "# Data that will be injected as a variable\n",
    "CITY_DATA = {\n",
    "    \"New York\": {\"population\": 8_336_817, \"area_sq_mi\": 302.6},\n",
    "    \"Los Angeles\": {\"population\": 3_979_576, \"area_sq_mi\": 468.7},\n",
    "    \"Chicago\": {\"population\": 2_693_976, \"area_sq_mi\": 227.3},\n",
    "    \"Houston\": {\"population\": 2_304_580, \"area_sq_mi\": 670.6},\n",
    "}\n",
    "\n",
    "\n",
    "rlm_tools = RLM(\n",
    "    backend=BACKEND,\n",
    "    backend_kwargs=BACKEND_KWARGS,\n",
    "    environment=\"local\",\n",
    "    max_depth=1,\n",
    "    custom_tools={\n",
    "        \"word_count\": {\n",
    "            \"tool\": word_count,\n",
    "            \"description\": \"Count words in a string\",\n",
    "        },\n",
    "        \"extract_numbers\": {\n",
    "            \"tool\": extract_numbers,\n",
    "            \"description\": \"Extract all numbers from a text string\",\n",
    "        },\n",
    "        \"celsius_to_fahrenheit\": {\n",
    "            \"tool\": celsius_to_fahrenheit,\n",
    "            \"description\": \"Convert a temperature from Celsius to Fahrenheit\",\n",
    "        },\n",
    "        \"CITY_DATA\": {\n",
    "            \"tool\": CITY_DATA,\n",
    "            \"description\": \"Dict of US cities with population and area data\",\n",
    "        },\n",
    "    },\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "result = rlm_tools.completion(\n",
    "    \"Using the CITY_DATA, compute the population density (people per square mile) \"\n",
    "    \"for each city. Which city has the highest density? \"\n",
    "    \"Return a dict mapping city name to density, sorted by density descending, via FINAL_VAR.\"\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"Response: {result.response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom tools enable domain-specific RLMs\n",
    "\n",
    "By injecting functions and data, you can build RLMs specialized for:\n",
    "- **Data analysis** — pass in database query functions\n",
    "- **Scientific computing** — pass in simulation functions\n",
    "- **Web scraping** — pass in HTTP fetch functions\n",
    "- **Business logic** — pass in pricing calculators, inventory lookups, etc.\n",
    "\n",
    "The model discovers these tools through descriptions in its system prompt and uses them naturally in its REPL code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Logging & Trajectory Inspection <a id='9-logging--trajectory-inspection'></a>\n",
    "\n",
    "The `RLMLogger` captures complete execution trajectories — every iteration, every code block, every sub-call. This is invaluable for debugging and understanding how the model reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Full logging example ──\n",
    "full_logger = RLMLogger()  # log_dir=\"./logs\" to also save to disk\n",
    "\n",
    "rlm_full = RLM(\n",
    "    backend=BACKEND,\n",
    "    backend_kwargs=BACKEND_KWARGS,\n",
    "    environment=\"local\",\n",
    "    max_depth=1,\n",
    "    max_iterations=5,\n",
    "    logger=full_logger,\n",
    "    verbose=False,  # Quiet mode — we'll inspect via the logger instead\n",
    ")\n",
    "\n",
    "result = rlm_full.completion(\n",
    "    \"Write a function that checks if a number is prime. \"\n",
    "    \"Test it on the numbers 1 through 20. \"\n",
    "    \"Return the list of primes via FINAL_VAR.\"\n",
    ")\n",
    "\n",
    "print(f\"Response: {result.response}\")\n",
    "print(f\"Execution time: {result.execution_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Detailed trajectory analysis ──\n",
    "if result.metadata:\n",
    "    traj = result.metadata\n",
    "    \n",
    "    # Run-level metadata\n",
    "    run_meta = traj.get(\"run_metadata\", {})\n",
    "    print(\"Run Metadata:\")\n",
    "    for key, value in run_meta.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    print()\n",
    "    \n",
    "    # Iteration details\n",
    "    iterations = traj.get(\"iterations\", [])\n",
    "    print(f\"Iterations: {len(iterations)}\")\n",
    "    print()\n",
    "    \n",
    "    total_code_blocks = 0\n",
    "    total_lines_of_code = 0\n",
    "    \n",
    "    for i, iteration in enumerate(iterations):\n",
    "        code_blocks = iteration.get(\"code_blocks\", [])\n",
    "        total_code_blocks += len(code_blocks)\n",
    "        print(f\"  Iteration {i + 1}: {len(code_blocks)} code block(s)\")\n",
    "        for j, cb in enumerate(code_blocks):\n",
    "            code = cb.get(\"code\", \"\")\n",
    "            lines = code.count(\"\\n\") + 1\n",
    "            total_lines_of_code += lines\n",
    "            repl_result = cb.get(\"result\", {})\n",
    "            success = not repl_result.get(\"stderr\", \"\")\n",
    "            status = \"OK\" if success else \"ERROR\"\n",
    "            print(f\"    Block {j+1}: {lines} lines [{status}]\")\n",
    "            if not success:\n",
    "                print(f\"      stderr: {repl_result['stderr'][:200]}\")\n",
    "    \n",
    "    print(f\"\\nTotal: {total_code_blocks} code blocks, {total_lines_of_code} lines of code\")\n",
    "else:\n",
    "    print(\"No metadata captured (logger may not have been attached).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving trajectories to disk\n",
    "\n",
    "To save trajectories for later analysis or for use with the RLM [web visualizer](https://github.com/alexzhang13/rlm/tree/main/visualizer):\n",
    "\n",
    "```python\n",
    "logger = RLMLogger(log_dir=\"./logs\")\n",
    "```\n",
    "\n",
    "This writes JSONL files that can be loaded into the visualizer for interactive exploration of the full execution tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Exercises <a id='10-exercises'></a>\n",
    "\n",
    "Now it's your turn! Try these exercises to deepen your understanding of RLM.\n",
    "\n",
    "### Exercise 1: Context Processing\n",
    "\n",
    "Pass a long text as the `context` and have the RLM process it programmatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Exercise 1: Using the context variable ──\n",
    "\n",
    "long_text = \"\"\"\n",
    "The history of computing is a fascinating journey. Charles Babbage conceived the\n",
    "Analytical Engine in 1837, which contained many features of modern computers.\n",
    "Ada Lovelace wrote what is considered the first computer program for this machine.\n",
    "In 1936, Alan Turing published his paper on computable numbers, laying the\n",
    "theoretical foundation for computer science. The first electronic general-purpose\n",
    "computer, ENIAC, was completed in 1945. It weighed 30 tons and occupied 1,800\n",
    "square feet. The transistor was invented at Bell Labs in 1947, revolutionizing\n",
    "electronics. The integrated circuit followed in 1958, leading to the\n",
    "miniaturization of computing. The first microprocessor, the Intel 4004, was\n",
    "released in 1971. The personal computer revolution began in the late 1970s with\n",
    "machines like the Apple II and the IBM PC. Tim Berners-Lee invented the World\n",
    "Wide Web in 1989. The smartphone era began with the iPhone in 2007. Today,\n",
    "artificial intelligence and machine learning are transforming computing once again.\n",
    "\"\"\"\n",
    "\n",
    "rlm_ctx = RLM(\n",
    "    backend=BACKEND,\n",
    "    backend_kwargs=BACKEND_KWARGS,\n",
    "    environment=\"local\",\n",
    "    max_depth=1,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# The context variable is available as `context` inside the REPL\n",
    "result = rlm_ctx.completion(\n",
    "    \"The variable `context` contains a passage about computing history. \"\n",
    "    \"Extract every year mentioned and the event associated with it. \"\n",
    "    \"Return a dict mapping year (int) to event (str) via FINAL_VAR.\",\n",
    "    context=long_text,\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"Response: {result.response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Recursive Document Summarization\n",
    "\n",
    "Use `rlm_query_batched()` to implement a map-reduce style summarizer. The parent should chunk text and delegate summarization to child RLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Exercise 2: Recursive summarization ──\n",
    "# TODO: Create an RLM with depth=2 and write a prompt that instructs\n",
    "# the parent to:\n",
    "#   1. Split the context into 3 equal parts\n",
    "#   2. Use rlm_query_batched() to summarize each part\n",
    "#   3. Combine the summaries into one final summary\n",
    "#   4. Return via FINAL_VAR\n",
    "\n",
    "# Your code here:\n",
    "# rlm_summarizer = RLM(...)\n",
    "# result = rlm_summarizer.completion(\"...\", context=long_text)\n",
    "\n",
    "print(\"Complete this exercise by implementing recursive summarization!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Custom Tool Integration\n",
    "\n",
    "Create a custom tool that simulates a database, and have the RLM query it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Exercise 3: Build a database tool ──\n",
    "# TODO: Create a function that simulates a SQL query on in-memory data.\n",
    "# Inject it as a custom tool and have the RLM answer questions about the data.\n",
    "\n",
    "# Hint:\n",
    "# def query_db(table: str, column: str, condition: str = None) -> list:\n",
    "#     ...\n",
    "#\n",
    "# rlm_db = RLM(..., custom_tools={\"query_db\": {\"tool\": query_db, \"description\": \"...\"}})\n",
    "\n",
    "print(\"Complete this exercise by building a database query tool!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Error Recovery\n",
    "\n",
    "What happens when the model writes buggy code? RLM feeds the error back, and the model can fix it. Test this by giving a tricky prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Exercise 4: Observe error recovery ──\n",
    "\n",
    "rlm_err = RLM(\n",
    "    backend=BACKEND,\n",
    "    backend_kwargs=BACKEND_KWARGS,\n",
    "    environment=\"local\",\n",
    "    max_depth=1,\n",
    "    max_iterations=5,\n",
    "    logger=RLMLogger(),\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# This prompt intentionally asks for something that might cause errors\n",
    "result = rlm_err.completion(\n",
    "    \"Try to import a library called 'nonexistent_lib'. \"\n",
    "    \"When that fails, catch the error and instead compute factorial(10) \"\n",
    "    \"using the math library. Return the result via FINAL_VAR.\"\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"Response: {result.response}\")\n",
    "print(f\"Execution time: {result.execution_time:.2f}s\")\n",
    "\n",
    "# Check how many iterations were needed\n",
    "if result.metadata:\n",
    "    iters = len(result.metadata.get(\"iterations\", []))\n",
    "    print(f\"Iterations needed: {iters}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. References <a id='11-references'></a>\n",
    "\n",
    "1. **Zhang, A.L., Kraska, T., & Khattab, O.** (2025). *Recursive Language Models.* arXiv:2512.24601. MIT OASYS Lab.\n",
    "\n",
    "2. **RLM GitHub Repository:** [github.com/alexzhang13/rlm](https://github.com/alexzhang13/rlm)\n",
    "\n",
    "3. **RLM Visualizer:** Interactive web tool for exploring RLM execution trajectories — see the `visualizer/` directory in the repo.\n",
    "\n",
    "4. **RLM on PyPI:** `pip install rlms`\n",
    "\n",
    "---\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **RLM is an inference paradigm**, not a model architecture — it works with any LLM backend\n",
    "- The REPL loop gives models **programmatic control** over their reasoning\n",
    "- **Recursive sub-calls** enable divide-and-conquer over arbitrarily long inputs\n",
    "- **Compaction** manages context windows automatically for long-running tasks\n",
    "- **Custom tools** let you build domain-specific RLM applications\n",
    "- **Trajectory logging** provides full transparency into the model's reasoning process\n",
    "\n",
    "Happy hacking!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 4,
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
